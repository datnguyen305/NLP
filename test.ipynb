{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bded996",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c484501",
   "metadata": {},
   "source": [
    "# TRANSFORMER WITH WORD-BASED TOKENIZER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b13c7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerPhonemeEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, d_model: int, max_len: int = 512, padding_idx: int = 0, dropout_rate: float = 0.1):\n",
    "        \"\"\"\n",
    "        Khởi tạo lớp Embedding phù hợp với Transformer.\n",
    "\n",
    "        Args:\n",
    "            vocab_size (int): Kích thước từ điển của tất cả các âm vị.\n",
    "            d_model (int): Kích thước cuối cùng của vector đầu ra (kích thước ẩn của Transformer).\n",
    "                           Phải chia hết cho 4 nếu dùng phương pháp Concatenate.\n",
    "            max_len (int): Chiều dài tối đa của câu.\n",
    "            padding_idx (int): Chỉ số của token PAD (thường là 0).\n",
    "            dropout_rate (float): Tỷ lệ Dropout.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # 1. Phoneme Embedding Setup\n",
    "        # Kích thước embedding cho mỗi thành phần âm vị\n",
    "        # Giả sử ta dùng Concatenate, mỗi thành phần sẽ có kích thước d_model / 4\n",
    "        assert d_model % 4 == 0, \"d_model phải chia hết cho 4 cho Concatenate.\"\n",
    "        self.phoneme_embed_dim = d_model // 4\n",
    "        \n",
    "        self.onset_embed = nn.Embedding(vocab_size, self.phoneme_embed_dim, padding_idx=padding_idx)\n",
    "        self.medial_embed = nn.Embedding(vocab_size, self.phoneme_embed_dim, padding_idx=padding_idx)\n",
    "        self.nucleus_embed = nn.Embedding(vocab_size, self.phoneme_embed_dim, padding_idx=padding_idx)\n",
    "        self.coda_embed = nn.Embedding(vocab_size, self.phoneme_embed_dim, padding_idx=padding_idx)\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # 2. Positional Encoding Setup (Được học)\n",
    "        # Tốt hơn là dùng Positional Encoding cố định (như Transformer gốc)\n",
    "        self.pos_encoder = self._get_fixed_positional_encoding(d_model, max_len)\n",
    "        \n",
    "        # 3. Dropout\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "\n",
    "    def _get_fixed_positional_encoding(self, d_model: int, max_len: int) -> nn.Parameter:\n",
    "        \"\"\"\n",
    "        Tạo Positional Encoding cố định (dạng sin/cos) theo kiến trúc Transformer gốc.\n",
    "        \"\"\"\n",
    "        # Tạo ma trận PE (max_len, d_model)\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # Thêm một chiều Batch, biến thành Parameter để nó được lưu trữ (nhưng không được học)\n",
    "        pe = pe.unsqueeze(0) \n",
    "        return nn.Parameter(pe, requires_grad=False)\n",
    "        \n",
    "\n",
    "    def forward(self, input_tensor: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_tensor: Tensor có shape (Batch_Size, Seq_Len, 4).\n",
    "                          4 cột là ID của Onset, Medial, Nucleus, Coda.\n",
    "                          \n",
    "        Returns:\n",
    "            Tensor có shape (Batch_Size, Seq_Len, d_model) sẵn sàng cho Decoder/Encoder.\n",
    "        \"\"\"\n",
    "        B, L, _ = input_tensor.shape\n",
    "        \n",
    "        # 1. PHONEME EMBEDDING (Học Biểu diễn Âm vị)\n",
    "        \n",
    "        # Tách input tensor thành 4 tensor riêng biệt (chỉ số 0, 1, 2, 3)\n",
    "        onset_ids = input_tensor[..., 0]\n",
    "        medial_ids = input_tensor[..., 1]\n",
    "        nucleus_ids = input_tensor[..., 2]\n",
    "        coda_ids = input_tensor[..., 3]\n",
    "        \n",
    "        # Lấy Embedding cho từng thành phần\n",
    "        onset_embedded = self.onset_embed(onset_ids)      \n",
    "        medial_embedded = self.medial_embed(medial_ids)    \n",
    "        nucleus_embedded = self.nucleus_embed(nucleus_ids)  \n",
    "        coda_embedded = self.coda_embed(coda_ids)          \n",
    "        \n",
    "        # CONCATENATE 4 vector lại (B, L, d_model)\n",
    "        phoneme_embedding = torch.cat(\n",
    "            (onset_embedded, medial_embedded, nucleus_embedded, coda_embedded), \n",
    "            dim=-1 \n",
    "        )\n",
    "        \n",
    "        # 2. POSITIONAL ENCODING (Thêm Vị trí)\n",
    "        # Lấy Positional Encoding cho chiều dài hiện tại L\n",
    "        # \n",
    "        positional_encoding = self.pos_encoder[:, :L, :]\n",
    "        \n",
    "        # Cộng Positional Encoding vào Phoneme Embedding\n",
    "        output = phoneme_embedding + positional_encoding\n",
    "        \n",
    "        # 3. DROPOUT\n",
    "        return self.dropout(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70932ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def generate_square_subsequent_mask(sz, device):\n",
    "    \"\"\"\n",
    "    Tạo mask hình tam giác vuông để che các từ tương lai.\n",
    "    Input: sz (độ dài câu tóm tắt)\n",
    "    Output: Tensor (sz, sz) chứa 0 và -inf\n",
    "    \"\"\"\n",
    "    mask = (torch.triu(torch.ones((sz, sz), device=device)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7483b8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# Import các module bạn đã viết trước đó\n",
    "# from models.transformer.embedding import TransformerPhonemeEmbedding\n",
    "# from models.decoder import PhonemeDecoder\n",
    "\n",
    "class ViSeq2SeqTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, max_len, device, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "        # 1. EMBEDDING & ENCODER\n",
    "        # Embedding cho Source (Văn bản gốc)\n",
    "        self.src_embedding = TransformerPhonemeEmbedding(vocab_size, d_model, max_len, padding_idx=0, dropout_rate=dropout)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "\n",
    "        # 2. DECODER (Đã bao gồm Embedding cho Target bên trong class PhonemeDecoder ta thiết kế trước đó)\n",
    "        # Lưu ý: Chúng ta cần sửa lại PhonemeDecoder một chút để nó nhận embedding từ bên ngoài hoặc tự tạo.\n",
    "        # Để tiện nhất, ta khai báo Embedding Target riêng ở đây cho đồng bộ.\n",
    "        self.tgt_embedding = TransformerPhonemeEmbedding(vocab_size, d_model, max_len, padding_idx=0, dropout_rate=dropout)\n",
    "        \n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, batch_first=True)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n",
    "\n",
    "        # 3. GENERATOR HEADS (4 đầu ra)\n",
    "        self.onset_head = nn.Linear(d_model, vocab_size)\n",
    "        self.medial_head = nn.Linear(d_model, vocab_size)\n",
    "        self.nucleus_head = nn.Linear(d_model, vocab_size)\n",
    "        self.coda_head = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def create_padding_mask(self, tensor):\n",
    "        \"\"\"Tạo mask cho vị trí padding (Onset == 0)\"\"\"\n",
    "        # tensor: (Batch, Seq_Len, 4) -> Mask: (Batch, Seq_Len)\n",
    "        return (tensor[..., 0] == 0)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        \"\"\"\n",
    "        src: (Batch, Src_Len, 4)\n",
    "        tgt: (Batch, Tgt_Len, 4) - Lưu ý: Đây là Decoder Input (đã bỏ <eos>)\n",
    "        \"\"\"\n",
    "        \n",
    "        # --- BƯỚC 1: TẠO MASK ---\n",
    "        # Mask che padding cho Source và Target\n",
    "        src_padding_mask = self.create_padding_mask(src).to(self.device)\n",
    "        tgt_padding_mask = self.create_padding_mask(tgt).to(self.device)\n",
    "        \n",
    "        # Mask che tương lai cho Target (Causal Mask)\n",
    "        tgt_seq_len = tgt.shape[1]\n",
    "        tgt_mask = generate_square_subsequent_mask(tgt_seq_len, self.device)\n",
    "        \n",
    "        # --- BƯỚC 2: ENCODER ---\n",
    "        # Embed Source\n",
    "        src_emb = self.src_embedding(src) # (Batch, Src_Len, D_Model)\n",
    "        \n",
    "        # Qua Encoder\n",
    "        memory = self.transformer_encoder(\n",
    "            src=src_emb, \n",
    "            src_key_padding_mask=src_padding_mask\n",
    "        )\n",
    "        \n",
    "        # --- BƯỚC 3: DECODER ---\n",
    "        # Embed Target\n",
    "        tgt_emb = self.tgt_embedding(tgt) # (Batch, Tgt_Len, D_Model)\n",
    "        \n",
    "        # Qua Decoder\n",
    "        # memory là output của encoder\n",
    "        dec_output = self.transformer_decoder(\n",
    "            tgt=tgt_emb,\n",
    "            memory=memory,\n",
    "            tgt_mask=tgt_mask,                   # Che tương lai\n",
    "            tgt_key_padding_mask=tgt_padding_mask, # Che padding của target\n",
    "            memory_key_padding_mask=src_padding_mask # Che padding của memory (source)\n",
    "        )\n",
    "        \n",
    "        # --- BƯỚC 4: DỰ ĐOÁN (4 Nhánh) ---\n",
    "        logits_onset = self.onset_head(dec_output)\n",
    "        logits_medial = self.medial_head(dec_output)\n",
    "        logits_nucleus = self.nucleus_head(dec_output)\n",
    "        logits_coda = self.coda_head(dec_output)\n",
    "        \n",
    "        return logits_onset, logits_medial, logits_nucleus, logits_coda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560269ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhonemeLoss(nn.Module):\n",
    "    def __init__(self, padding_idx=0):\n",
    "        super().__init__()\n",
    "        # ignore_index=padding_idx để không tính loss cho các token padding\n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index=padding_idx)\n",
    "    \n",
    "    def forward(self, outputs, targets):\n",
    "        \"\"\"\n",
    "        outputs: Tuple (logit_onset, logit_medial, logit_nucleus, logit_coda)\n",
    "                 Mỗi cái có shape (Batch, Seq_Len, Vocab_Size)\n",
    "                 \n",
    "        targets: Tensor (Batch, Seq_Len, 4) - Ground Truth Label\n",
    "        \"\"\"\n",
    "        p_onset, p_medial, p_nucleus, p_coda = outputs\n",
    "        \n",
    "        # Tách target ra 4 phần tương ứng\n",
    "        t_onset = targets[..., 0]\n",
    "        t_medial = targets[..., 1]\n",
    "        t_nucleus = targets[..., 2]\n",
    "        t_coda = targets[..., 3]\n",
    "        \n",
    "        # Tính Loss cho từng phần\n",
    "        # CrossEntropy yêu cầu input (Batch, Class, Seq) hoặc flatten\n",
    "        # Ta reshape: (Batch * Seq_Len, Vocab_Size) vs (Batch * Seq_Len)\n",
    "        \n",
    "        vocab_size = p_onset.shape[-1]\n",
    "        \n",
    "        loss_onset = self.criterion(p_onset.reshape(-1, vocab_size), t_onset.reshape(-1))\n",
    "        loss_medial = self.criterion(p_medial.reshape(-1, vocab_size), t_medial.reshape(-1))\n",
    "        loss_nucleus = self.criterion(p_nucleus.reshape(-1, vocab_size), t_nucleus.reshape(-1))\n",
    "        loss_coda = self.criterion(p_coda.reshape(-1, vocab_size), t_coda.reshape(-1))\n",
    "        \n",
    "        # Tổng Loss\n",
    "        total_loss = loss_onset + loss_medial + loss_nucleus + loss_coda\n",
    "        \n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01bb7ee5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'text_sum_dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-4216407751.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# --- IMPORT CÁC MODULE CỦA BẠN ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtext_sum_dataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mViTextSumDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollate_fn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mViCollator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mvocabs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviword_vocab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mViWordVocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'text_sum_dataset'",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm # Thư viện tạo thanh loading\n",
    "\n",
    "# --- IMPORT CÁC MODULE CỦA BẠN ---\n",
    "from text_sum_dataset import ViTextSumDataset\n",
    "from collate_fn_phoneme import ViCollator\n",
    "from vocabs.viword_vocab import ViWordVocab \n",
    "from configs.phoneme_config import Config\n",
    "\n",
    "# Giả sử bạn đã lưu Model và Loss vào file models.py hoặc định nghĩa ngay bên trên\n",
    "# from models import ViSeq2SeqTransformer, PhonemeLoss \n",
    "# (Nếu chưa tách file thì phải paste class Model và Loss vào đây trước)\n",
    "\n",
    "def train():\n",
    "    # 1. Khởi tạo Config\n",
    "    config = Config()\n",
    "    \n",
    "    # 2. Khởi tạo Vocab & Dataset\n",
    "    print(\"Đang xây dựng từ điển...\")\n",
    "    vocab_obj = ViWordVocab(config)\n",
    "    \n",
    "    # ⚠️ QUAN TRỌNG: Cập nhật VOCAB_SIZE vào config sau khi vocab đã chạy xong\n",
    "    config.VOCAB_SIZE = len(vocab_obj.itos)\n",
    "    print(f\"Vocab Size: {config.VOCAB_SIZE}\")\n",
    "\n",
    "    # Cài đặt đường dẫn train\n",
    "    config.path = config.TRAIN \n",
    "    print(\"Đang tải dữ liệu Train...\")\n",
    "    train_dataset = ViTextSumDataset(config, vocab_obj)\n",
    "\n",
    "    # 3. Khởi tạo DataLoader\n",
    "    collator = ViCollator(padding_idx=vocab_obj.padding_idx)\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=config.BATCH_SIZE, \n",
    "        shuffle=True, \n",
    "        num_workers=2, \n",
    "        collate_fn=collator\n",
    "    )\n",
    "\n",
    "    # 4. Khởi tạo Model\n",
    "    print(\"Đang khởi tạo Model...\")\n",
    "    model = ViSeq2SeqTransformer(\n",
    "        vocab_size=config.VOCAB_SIZE, # Lấy từ biến vừa cập nhật\n",
    "        d_model=config.D_MODEL,\n",
    "        nhead=config.N_HEAD,\n",
    "        num_encoder_layers=config.NUM_ENCODER_LAYERS,\n",
    "        num_decoder_layers=config.NUM_DECODER_LAYERS,\n",
    "        dim_feedforward=config.DIM_FEEDFORWARD,\n",
    "        max_len=config.MAX_LEN,\n",
    "        device=config.DEVICE,\n",
    "        dropout=config.DROPOUT\n",
    "    ).to(config.DEVICE)\n",
    "\n",
    "    # 5. Optimizer & Loss\n",
    "    # padding_idx=0 để không tính loss cho phần đệm\n",
    "    criterion = PhonemeLoss(padding_idx=vocab_obj.padding_idx)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config.LEARNING_RATE)\n",
    "\n",
    "    # 6. TRAINING LOOP\n",
    "    print(\"Bắt đầu huấn luyện...\")\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(config.NUM_EPOCHS):\n",
    "        # Tạo thanh loading bar cho Epoch hiện tại\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config.NUM_EPOCHS}\", unit=\"batch\")\n",
    "        \n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch in progress_bar:\n",
    "            # Chuyển dữ liệu sang GPU/CPU\n",
    "            src = batch[\"src\"].to(config.DEVICE)             # (B, Src_Len, 4)\n",
    "            tgt_input = batch[\"decoder_input\"].to(config.DEVICE) # (B, Tgt_Len, 4)\n",
    "            labels = batch[\"labels\"].to(config.DEVICE)       # (B, Tgt_Len, 4)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(src, tgt_input)\n",
    "            \n",
    "            # Tính Loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Clip grad norm để tránh bùng nổ gradient (tùy chọn nhưng khuyên dùng cho Transformer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            # Cập nhật thông tin lên thanh loading\n",
    "            current_loss = loss.item()\n",
    "            total_loss += current_loss\n",
    "            progress_bar.set_postfix(loss=f\"{current_loss:.4f}\")\n",
    "        \n",
    "        # In loss trung bình của cả epoch\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"Kết thúc Epoch {epoch+1} | Average Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        torch.save(model.state_dict(), f\"checkpoint_epoch_{epoch+1}.pt\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
