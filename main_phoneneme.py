import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from tqdm import tqdm
import argparse
import importlib.util 
# Gi·∫£ ƒë·ªãnh c√°c import sau l√† ƒë√∫ng
from vocab.text_sum_dataset_phoneme import ViTextSumDataset
from collate_fn.collate_fn_phoneme import ViCollator
from vocabs.viword_vocab import ViWordVocab 
from configs.phoneme_config import Config 
from models.transformer_phoneneme import ViSeq2SeqTransformer
from losses.phoneneme_loss import PhonemeLoss


def initialize_components(config: Config) -> tuple:
    """Kh·ªüi t·∫°o Vocab, c·∫≠p nh·∫≠t config, v√† kh·ªüi t·∫°o Model."""
    print("ƒêang x√¢y d·ª±ng t·ª´ ƒëi·ªÉn...")
    vocab_obj = ViWordVocab(config)
    
    # C·∫≠p nh·∫≠t VOCAB_SIZE
    config.VOCAB_SIZE = len(vocab_obj.itos)
    print(f"Vocab Size: {config.VOCAB_SIZE}")

    print("ƒêang kh·ªüi t·∫°o Model...")
    model = ViSeq2SeqTransformer(
        vocab_size=config.VOCAB_SIZE, 
        d_model=config.D_MODEL,
        nhead=config.N_HEAD,
        num_encoder_layers=config.NUM_ENCODER_LAYERS,
        num_decoder_layers=config.NUM_DECODER_LAYERS,
        dim_feedforward=config.DIM_FEEDFORWARD,
        max_len=config.MAX_LEN,
        device=config.DEVICE,
        dropout=config.DROPOUT
    ).to(config.DEVICE)
    
    criterion = PhonemeLoss(padding_idx=vocab_obj.padding_idx)
    optimizer = torch.optim.Adam(model.parameters(), lr=config.LEARNING_RATE)
    
    return vocab_obj, model, criterion, optimizer

def train_model(config: Config, vocab_obj: ViWordVocab, model: nn.Module, criterion: nn.Module, optimizer: torch.optim.Optimizer):
    """
    H√†m hu·∫•n luy·ªán m√¥ h√¨nh.
    """
    # C√†i ƒë·∫∑t ƒë∆∞·ªùng d·∫´n train v√† DataLoader
    config.path = config.TRAIN 
    print(f"ƒêang t·∫£i d·ªØ li·ªáu Train t·ª´: {config.path}")
    train_dataset = ViTextSumDataset(config, vocab_obj)
    collator = ViCollator(padding_idx=vocab_obj.padding_idx)
    train_loader = DataLoader(
        train_dataset, 
        batch_size=config.BATCH_SIZE, 
        shuffle=True, 
        num_workers=2, 
        collate_fn=collator
    )

    # B·∫Øt ƒë·∫ßu v√≤ng l·∫∑p hu·∫•n luy·ªán
    print("B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán...")
    model.train()
    
    for epoch in range(config.NUM_EPOCHS):
        progress_bar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{config.NUM_EPOCHS}", unit="batch")
        total_loss = 0
        
        for batch in progress_bar:
            # Chuy·ªÉn d·ªØ li·ªáu sang GPU/CPU
            src = batch["src"].to(config.DEVICE)
            tgt_input = batch["decoder_input"].to(config.DEVICE) 
            labels = batch["labels"].to(config.DEVICE)
            
            optimizer.zero_grad()
            
            # Forward pass
            outputs = model(src, tgt_input) # (B, Tgt_Len, 4, Vocab_Size)
            
            # T√≠nh Loss
            loss = criterion(outputs, labels)
            
            # Backward pass
            loss.backward()
            
            # Clip grad norm
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            optimizer.step()
            
            # C·∫≠p nh·∫≠t th√¥ng tin
            current_loss = loss.item()
            total_loss += current_loss
            progress_bar.set_postfix(loss=f"{current_loss:.4f}")
        
        # In loss trung b√¨nh c·ªßa c·∫£ epoch
        avg_loss = total_loss / len(train_loader)
        print(f"‚úÖ K·∫øt th√∫c Epoch {epoch+1} | Average Loss: {avg_loss:.4f}")
        
        # L∆∞u checkpoint
        # S·ª≠ d·ª•ng bi·∫øn CHECKPOINT_PATH t·ª´ config
        torch.save(model.state_dict(), f"{config.CHECKPOINT_PATH}_{epoch+1}.pt")


def evaluate_model(config: Config, vocab_obj: ViWordVocab, model: nn.Module, criterion: nn.Module, data_path: str) -> float:
    """
    H√†m ƒë√°nh gi√° m√¥ h√¨nh tr√™n t·∫≠p DEV/TEST.
    Tr·∫£ v·ªÅ Average Loss.
    """
    # C√†i ƒë·∫∑t ƒë∆∞·ªùng d·∫´n v√† DataLoader
    config.path = data_path
    print(f"ƒêang t·∫£i d·ªØ li·ªáu ƒê√°nh gi√° t·ª´: {config.path}")
    eval_dataset = ViTextSumDataset(config, vocab_obj)
    collator = ViCollator(padding_idx=vocab_obj.padding_idx)
    eval_loader = DataLoader(
        eval_dataset, 
        batch_size=config.BATCH_SIZE, 
        shuffle=False, # Kh√¥ng c·∫ßn shuffle khi ƒë√°nh gi√°
        num_workers=2, 
        collate_fn=collator
    )

    model.eval() # Chuy·ªÉn sang ch·∫ø ƒë·ªô ƒë√°nh gi√°
    total_loss = 0
    
    with torch.no_grad():
        progress_bar = tqdm(eval_loader, desc=f"ƒê√°nh gi√° ({'DEV' if 'dev' in data_path.lower() else 'TEST'})", unit="batch")
        for batch in progress_bar:
            src = batch["src"].to(config.DEVICE)
            tgt_input = batch["decoder_input"].to(config.DEVICE)
            labels = batch["labels"].to(config.DEVICE)
            
            # Forward pass
            outputs = model(src, tgt_input)
            
            # T√≠nh Loss
            loss = criterion(outputs, labels)
            total_loss += loss.item()
            progress_bar.set_postfix(loss=f"{loss.item():.4f}")

    avg_loss = total_loss / len(eval_loader)
    print(f"‚ú® ƒê√°nh gi√° ho√†n t·∫•t | Average Loss: {avg_loss:.4f}")
    return avg_loss

def generate_summary(config: Config, vocab_obj: ViWordVocab, model: nn.Module, source_text: str, max_len: int = 50) -> str:

    model.eval()
    
    # 1. M√£ h√≥a vƒÉn b·∫£n ngu·ªìn th√†nh tensor (Source: List[str] -> Tensor (1, Src_Len, 4))
    words = vocab_obj.preprocess_sentence(source_text)
    src_vec = vocab_obj.encode_caption(words).unsqueeze(0).to(config.DEVICE) # Th√™m dimension Batch
    
    # 2. Kh·ªüi t·∫°o ƒë·∫ßu v√†o cho Decoder
    # B·∫Øt ƒë·∫ßu v·ªõi token BOS: (1, 1, 4) -> BOS + 3 PAD
    start_token = (vocab_obj.bos_idx, vocab_obj.padding_idx, vocab_obj.padding_idx, vocab_obj.padding_idx)
    tgt_tokens = torch.tensor(start_token).long().unsqueeze(0).unsqueeze(0).to(config.DEVICE) # (1, 1, 4)

    # 3. V√≤ng l·∫∑p sinh c√¢u (Greedy Search)
    for _ in range(max_len):
        # outputs: (1, current_len, 4, Vocab_Size)
        with torch.no_grad():
            outputs = model(src_vec, tgt_tokens)
        
        # L·∫•y token cu·ªëi c√πng ƒë∆∞·ª£c d·ª± ƒëo√°n: (1, 4, Vocab_Size)
        last_prediction = outputs[:, -1, :, :] 
        
        # T√¨m index c·ªßa phoneme c√≥ x√°c su·∫•t cao nh·∫•t cho 4 th√†nh ph·∫ßn (Onset, Medial, Nucleus, Coda)
        # predicted_phoneme_ids: (1, 4)
        predicted_phoneme_ids = last_prediction.argmax(dim=-1) 
        
        # N·∫øu th√†nh ph·∫ßn Onset l√† EOS_ID (t∆∞∆°ng ƒë∆∞∆°ng v·ªõi m·ªôt t·ª´ ƒë∆∞·ª£c d·ª± ƒëo√°n l√† EOS) -> K·∫øt th√∫c
        if predicted_phoneme_ids[0, 0].item() == vocab_obj.eos_idx:
            break
            
        # N·ªëi k·∫øt qu·∫£ d·ª± ƒëo√°n v√†o ƒë·∫ßu v√†o c·ªßa decoder cho b∆∞·ªõc ti·∫øp theo
        # predicted_phoneme_ids c√≥ shape (1, 4), c·∫ßn reshape th√†nh (1, 1, 4) ƒë·ªÉ concatenate
        tgt_tokens = torch.cat([tgt_tokens, predicted_phoneme_ids.unsqueeze(1)], dim=1)
        
    summary_vec = tgt_tokens.squeeze(0).cpu() # (Tgt_Len, 4)
    summary_text = vocab_obj.decode_caption(summary_vec, join_words=True)
    
    return summary_text


def load_config_from_file(config_name: str):
    """N·∫°p l·ªõp Config t·ª´ file Python ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh (v√≠ d·ª•: 'config_large')."""
    # 1. X√¢y d·ª±ng ƒë∆∞·ªùng d·∫´n file: config_name.py
    spec = importlib.util.spec_from_file_location("config_module", f"{config_name}.py")
    
    if spec is None:
        raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y file config: {config_name}.py")
        
    config_module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(config_module)
    
    # 2. L·∫•y l·ªõp Config t·ª´ module ƒë√£ n·∫°p
    return config_module.Config()

def main():
    # B∆∞·ªõc 0: Thi·∫øt l·∫≠p Argument Parser
    parser = argparse.ArgumentParser(description="Hu·∫•n luy·ªán m√¥ h√¨nh Text Summarization.")
    parser.add_argument(
        "--config", 
        type=str, 
        default="configs/phoneme_config", # Gi·∫£ ƒë·ªãnh ƒë√¢y l√† gi√° tr·ªã m·∫∑c ƒë·ªãnh ƒë√∫ng c·ªßa b·∫°n
        help="T√™n file c·∫•u h√¨nh (bao g·ªìm c·∫£ ƒë∆∞·ªùng d·∫´n t∆∞∆°ng ƒë·ªëi, kh√¥ng bao g·ªìm ph·∫ßn m·ªü r·ªông .py). V√≠ d·ª•: 'configs/phoneme_config'"
    )
    args = parser.parse_args()
    
    try:
        # N·∫°p v√† kh·ªüi t·∫°o Config t·ª´ tham s·ªë d√≤ng l·ªánh
        config = load_config_from_file(args.config)
        
        # Th√™m thu·ªôc t√≠nh l∆∞u checkpoint m·∫∑c ƒë·ªãnh v√†o config
        if not hasattr(config, 'CHECKPOINT_PATH'):
            config.CHECKPOINT_PATH = "checkpoint_epoch" 

        # 1. Kh·ªüi t·∫°o c√°c th√†nh ph·∫ßn
        vocab_obj, model, criterion, optimizer = initialize_components(config)
        
        # 2. Hu·∫•n luy·ªán m√¥ h√¨nh
        train_model(config, vocab_obj, model, criterion, optimizer)
        
        # 3. ƒê√°nh gi√° m√¥ h√¨nh tr√™n t·∫≠p DEV
        print("\n" + "="*50)
        print("B·∫Øt ƒë·∫ßu ƒê√°nh gi√° tr√™n t·∫≠p DEV")
        evaluate_model(config, vocab_obj, model, criterion, config.DEV)
        print("="*50 + "\n")

        # 4. V√≠ d·ª• sinh t√≥m t·∫Øt
        sample_text = "H√¥m nay, th·ªùi ti·∫øt t·∫°i th√†nh ph·ªë H·ªì Ch√≠ Minh r·∫•t ƒë·∫πp, n·∫Øng v√†ng r·ª±c r·ª° v√† kh√¥ng kh√≠ trong l√†nh, r·∫•t th√≠ch h·ª£p cho c√°c ho·∫°t ƒë·ªông ngo√†i tr·ªùi."
        print("üîç V√≠ d·ª• Sinh T√≥m T·∫Øt (Inference)")
        
        # T·∫£i checkpoint t·ªët nh·∫•t (ho·∫∑c cu·ªëi c√πng)
        try:
            checkpoint_file = f"{config.CHECKPOINT_PATH}_{config.NUM_EPOCHS}.pt"
            model.load_state_dict(torch.load(checkpoint_file))
            print(f"‚úÖ ƒê√£ t·∫£i checkpoint: {checkpoint_file}")
        except FileNotFoundError:
            # S·ª≠a l·ªói: In ra th√¥ng b√°o r√µ r√†ng khi kh√¥ng t√¨m th·∫•y file
            print(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file checkpoint ({checkpoint_file}). D√πng model cu·ªëi c√πng trong b·ªô nh·ªõ.")
        except Exception as e:
            # S·ª≠a l·ªói: In ra l·ªói n·∫øu c√≥ v·∫•n ƒë·ªÅ kh√°c khi t·∫£i state_dict
            print(f"‚ùå L·ªói khi t·∫£i checkpoint: {e}")
        
        # S·ª≠a l·ªói: generate_summary kh√¥ng c√≤n b·ªã ch·∫∑n b·ªüi kh·ªëi try...except l·ªõn n·ªØa
        summary = generate_summary(config, vocab_obj, model, sample_text)
        print(f"VƒÉn b·∫£n g·ªëc: {sample_text}")
        print(f"T√≥m t·∫Øt: {summary}")
        print("="*50)
        
        # 5. ƒê√°nh gi√° cu·ªëi c√πng tr√™n t·∫≠p TEST
        print("\n" + "!"*50)
        print("TI·∫æN H√ÄNH ƒê√ÅNH GI√Å CU·ªêI C√ôNG TR√äN T·∫¨P TEST")
        # S·ª≠a l·ªói: evaluate_model tr√™n TEST kh√¥ng c√≤n b·ªã ch·∫∑n n·ªØa
        evaluate_model(config, vocab_obj, model, criterion, config.TEST)
        print("!"*50)

    except FileNotFoundError as e:
        # S·ª≠a l·ªói: In ra t√™n file b·ªã thi·∫øu r√µ r√†ng
        print(f"L·ªñI KH·ªûI T·∫†O: Kh√¥ng t√¨m th·∫•y file. {e}. Vui l√≤ng ki·ªÉm tra t√™n file config v√† ƒë∆∞·ªùng d·∫´n.")
    except AttributeError as e: 
        # S·ª≠a l·ªói: In ra l·ªói thu·ªôc t√≠nh g·ªëc ƒë·ªÉ g·ª° l·ªói ch√≠nh x√°c
        print(f"L·ªñI C·∫§U H√åNH: Thi·∫øu thu·ªôc t√≠nh c·∫ßn thi·∫øt trong l·ªõp Config. L·ªói g·ªëc: {e}. Vui l√≤ng ki·ªÉm tra file config.")
    except Exception as e:
        print(f"L·ªñI KH√îNG X√ÅC ƒê·ªäNH: {e}")


if __name__ == '__main__':
    main()