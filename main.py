import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from tqdm import tqdm
import argparse
import importlib.util
import os

# Import c√°c module c·ªßa b·∫°n (H√£y ƒëi·ªÅu ch·ªânh ƒë∆∞·ªùng d·∫´n import n·∫øu kh√°c)
from vocab.text_sum_dataset import TextSumDataset      # Class Dataset chu·∫©n
from collate_fn.collate_fn import Collator         # Class Collator chu·∫©n
from vocab.vocab import Vocab                   # Class Vocab chu·∫©n
from models.transformer import Seq2SeqTransformer # Model Transformer chu·∫©n
from losses.loss import TextSumLoss             # Loss Function chu·∫©n

# N·∫øu file config n·∫±m ·ªü configs/config.py
# from configs.config import Config 

def load_config_from_file(config_name: str):
    """N·∫°p l·ªõp Config t·ª´ file Python ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh."""
    file_path = f"configs/{config_name}.py"
    if not os.path.exists(file_path):
         # Fallback n·∫øu ƒë·ªÉ file config c√πng c·∫•p
         file_path = f"{config_name}.py"
         
    spec = importlib.util.spec_from_file_location("config_module", file_path)
    if spec is None:
        raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y file config: {file_path}")
        
    config_module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(config_module)
    return config_module.Config()

def initialize_components(config) -> tuple:
    """Kh·ªüi t·∫°o Vocab, c·∫≠p nh·∫≠t config, v√† kh·ªüi t·∫°o Model."""
    print("üõ† ƒêang x√¢y d·ª±ng t·ª´ ƒëi·ªÉn (Vocab)...")
    vocab_obj = Vocab(config)
    
    # C·∫≠p nh·∫≠t VOCAB_SIZE v√†o config ƒë·ªÉ Model d√πng
    config.VOCAB_SIZE = vocab_obj.vocab_size
    print(f"‚úÖ Vocab Size: {config.VOCAB_SIZE}")

    print("üèó ƒêang kh·ªüi t·∫°o Model Transformer...")
    model = Seq2SeqTransformer(config, vocab_obj).to(config.DEVICE)
    
    # Kh·ªüi t·∫°o Loss function (c√≥ label smoothing)
    criterion = TextSumLoss(pad_idx=vocab_obj.pad_idx, label_smoothing=0.1).to(config.DEVICE)
    
    # Optimizer
    optimizer = torch.optim.Adam(model.parameters(), lr=config.LEARNING_RATE, betas=(0.9, 0.98), eps=1e-9)
    
    return vocab_obj, model, criterion, optimizer

def train_model(config, vocab_obj, model, criterion, optimizer):
    """
    H√†m hu·∫•n luy·ªán m√¥ h√¨nh.
    """
    # 1. Setup DataLoader
    config.path = config.TRAIN 
    print(f"üìÇ ƒêang t·∫£i d·ªØ li·ªáu Train t·ª´: {config.path}")
    
    train_dataset = TextSumDataset(config, vocab_obj)
    collator = Collator(pad_idx=vocab_obj.pad_idx)
    
    train_loader = DataLoader(
        train_dataset, 
        batch_size=config.BATCH_SIZE, 
        shuffle=True, 
        num_workers=2, 
        collate_fn=collator
    )

    print("üöÄ B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán...")
    model.train()
    
    for epoch in range(config.NUM_EPOCHS):
        progress_bar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{config.NUM_EPOCHS}", unit="batch")
        total_loss = 0
        
        for batch in progress_bar:
            # L·∫•y d·ªØ li·ªáu t·ª´ batch
            # Gi·∫£ s·ª≠ batch tr·∫£ v·ªÅ keys: 'input_ids' v√† 'label'
            src = batch["input_ids"].to(config.DEVICE)   # (Batch, Src_Len)
            trg = batch["label"].to(config.DEVICE)       # (Batch, Trg_Len) -> G·ªìm <BOS>...<EOS>

            # --- X·ª≠ l√Ω Shifted Target cho Transformer ---
            # Decoder Input: B·ªè token cu·ªëi (<EOS>) -> [<BOS>, A, B, C]
            tgt_input = trg[:, :-1]
            
            # Target Label: B·ªè token ƒë·∫ßu (<BOS>) -> [A, B, C, <EOS>]
            tgt_output = trg[:, 1:]

            optimizer.zero_grad()
            
            # Forward pass
            # Model nh·∫≠n src v√† tgt_input
            logits = model(src, tgt_input) # (Batch, Seq_Len, Vocab_Size)
            
            # T√≠nh Loss
            loss = criterion(logits, tgt_output)
            
            # Backward pass
            loss.backward()
            
            # Clip grad norm ƒë·ªÉ tr√°nh b√πng n·ªï gradient
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            optimizer.step()
            
            # Update progress bar
            current_loss = loss.item()
            total_loss += current_loss
            progress_bar.set_postfix(loss=f"{current_loss:.4f}")
        
        # K·∫øt th√∫c Epoch
        avg_loss = total_loss / len(train_loader)
        print(f"‚úÖ K·∫øt th√∫c Epoch {epoch+1} | Average Loss: {avg_loss:.4f}")
        
        # L∆∞u checkpoint
        if not os.path.exists("checkpoints"):
            os.makedirs("checkpoints")
        torch.save(model.state_dict(), f"checkpoints/checkpoint_epoch_{epoch+1}.pt")

def evaluate_model(config, vocab_obj, model, criterion, data_path: str) -> float:
    """
    H√†m ƒë√°nh gi√° m√¥ h√¨nh tr√™n t·∫≠p DEV/TEST.
    """
    # Setup DataLoader
    config.path = data_path
    print(f"üìÇ ƒêang t·∫£i d·ªØ li·ªáu ƒê√°nh gi√° t·ª´: {config.path}")
    
    eval_dataset = TextSumDataset(config, vocab_obj)
    collator = Collator(pad_idx=vocab_obj.pad_idx)
    
    eval_loader = DataLoader(
        eval_dataset, 
        batch_size=config.BATCH_SIZE, 
        shuffle=False, 
        num_workers=2, 
        collate_fn=collator
    )

    model.eval() 
    total_loss = 0
    
    with torch.no_grad():
        progress_bar = tqdm(eval_loader, desc=f"Evaluating", unit="batch")
        for batch in progress_bar:
            src = batch["input_ids"].to(config.DEVICE)
            trg = batch["label"].to(config.DEVICE)
            
            tgt_input = trg[:, :-1]
            tgt_output = trg[:, 1:]
            
            logits = model(src, tgt_input)
            
            loss = criterion(logits, tgt_output)
            total_loss += loss.item()
            progress_bar.set_postfix(loss=f"{loss.item():.4f}")

    avg_loss = total_loss / len(eval_loader)
    print(f"‚ú® ƒê√°nh gi√° ho√†n t·∫•t | Average Loss: {avg_loss:.4f}")
    return avg_loss

def generate_summary(config, vocab_obj, model, source_text: str, max_len: int = 100) -> str:
    """
    H√†m sinh t√≥m t·∫Øt s·ª≠ d·ª•ng Greedy Search (ƒê√£ ƒë∆°n gi·∫£n h√≥a cho standard Transformer).
    """
    model.eval()
    
    # 1. M√£ h√≥a vƒÉn b·∫£n
    # encode_sentence tr·∫£ v·ªÅ tensor 1D, c·∫ßn th√™m batch dim -> (1, Seq_Len)
    src_tensor = vocab_obj.encode_sentence(source_text).unsqueeze(0).to(config.DEVICE)
    
    # 2. G·ªçi h√†m predict c·ªßa model (Greedy Search)
    # H√†m n√†y tr·∫£ v·ªÅ tensor token IDs (kh√¥ng bao g·ªìm BOS)
    with torch.no_grad():
        # L∆∞u √Ω: C·∫ßn ƒë·∫£m b·∫£o class Model c·ªßa b·∫°n c√≥ h√†m `predict` nh∆∞ t√¥i ƒë√£ cung c·∫•p ·ªü comment tr∆∞·ªõc
        output_tensor = model.predict(src_tensor, max_len=max_len)
    
    # 3. Gi·∫£i m√£ v·ªÅ text
    # decode_sentence nh·∫≠n batch -> unsqueeze(0)
    summary = vocab_obj.decode_sentence(output_tensor.unsqueeze(0), join_words=True)
    
    return summary[0]

def main():
    # B∆∞·ªõc 0: Thi·∫øt l·∫≠p Argument Parser
    parser = argparse.ArgumentParser(description="Hu·∫•n luy·ªán m√¥ h√¨nh Transformer Summarization.")
    parser.add_argument(
        "--config", 
        type=str, 
        default="config", 
        help="T√™n file c·∫•u h√¨nh (v√≠ d·ª•: 'config' cho file config.py)"
    )
    parser.add_argument("--mode", type=str, default="train", choices=["train", "inference"], help="Ch·∫ø ƒë·ªô ch·∫°y")
    args = parser.parse_args()
    
    try:
        # N·∫°p Config
        config = load_config_from_file(args.config)
        
        # 1. Kh·ªüi t·∫°o components
        vocab_obj, model, criterion, optimizer = initialize_components(config)
        
        if args.mode == "train":
            # 2. Hu·∫•n luy·ªán
            train_model(config, vocab_obj, model, criterion, optimizer)
            
            # 3. ƒê√°nh gi√° tr√™n DEV
            print("\n" + "="*50)
            print("üîç B·∫Øt ƒë·∫ßu ƒê√°nh gi√° tr√™n t·∫≠p DEV")
            evaluate_model(config, vocab_obj, model, criterion, config.DEV)
            print("="*50 + "\n")

            # 4. Test th·ª≠ 1 c√¢u
            sample_text = "Tr√≠ tu·ªá nh√¢n t·∫°o ƒëang thay ƒë·ªïi th·∫ø gi·ªõi m·ªôt c√°ch nhanh ch√≥ng th√¥ng qua c√°c m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn."
            print("üìù V√≠ d·ª• Sinh T√≥m T·∫Øt (Sau khi train):")
            summary = generate_summary(config, vocab_obj, model, sample_text)
            print(f"G·ªëc: {sample_text}")
            print(f"T√≥m t·∫Øt: {summary}")
            
        elif args.mode == "inference":
            # Load checkpoint ƒë·ªÉ test
            checkpoint_path = "checkpoints/checkpoint_epoch_10.pt" # V√≠ d·ª•
            if os.path.exists(checkpoint_path):
                print(f"Load checkpoint: {checkpoint_path}")
                model.load_state_dict(torch.load(checkpoint_path, map_location=config.DEVICE))
            
            text = input("Nh·∫≠p vƒÉn b·∫£n c·∫ßn t√≥m t·∫Øt: ")
            summary = generate_summary(config, vocab_obj, model, text)
            print(f"T√≥m t·∫Øt: {summary}")

    except FileNotFoundError as e:
        print(f"‚ùå L·ªói File: {e}")
    except Exception as e:
        print(f"‚ùå L·ªói kh√¥ng mong mu·ªën: {e}")
        import traceback
        traceback.print_exc()

if __name__ == '__main__':
    main()